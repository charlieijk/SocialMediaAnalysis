{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified demo script for Social Media Sentiment Analysis Platform\n",
    "This version uses only basic dependencies to showcase core functionality.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794570a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab80d14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple Text Preprocessor\n",
    "class SimpleTextPreprocessor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            print(\"Downloading NLTK data...\")\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove user mentions and hashtags\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "        # Remove emojis and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized_tokens = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words and len(token) > 2\n",
    "        ]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize_and_lemmatize(cleaned_text)\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cc904",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple Sentiment Analyzer\n",
    "class SimpleSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "        self.preprocessor = SimpleTextPreprocessor()\n",
    "\n",
    "    def prepare_data(self, texts, labels):\n",
    "        # Preprocess texts\n",
    "        preprocessed_texts = [self.preprocessor.preprocess(text) for text in texts]\n",
    "\n",
    "        # Convert to TF-IDF features\n",
    "        X = self.vectorizer.fit_transform(preprocessed_texts)\n",
    "        y = np.array(labels)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        \"\"\"Train multiple sentiment analysis models\"\"\"\n",
    "        print(\"üîß Training Naive Bayes model...\")\n",
    "        nb_model = MultinomialNB()\n",
    "        nb_model.fit(X, y)\n",
    "        self.models['naive_bayes'] = nb_model\n",
    "\n",
    "        print(\"üîß Training SVM model...\")\n",
    "        svm_model = SVC(probability=True, random_state=42)\n",
    "        svm_model.fit(X, y)\n",
    "        self.models['svm'] = svm_model\n",
    "\n",
    "        print(\"üîß Training Logistic Regression model...\")\n",
    "        lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        lr_model.fit(X, y)\n",
    "        self.models['logistic_regression'] = lr_model\n",
    "\n",
    "    def predict(self, texts, model_name):\n",
    "        \"\"\"Make predictions using specified model\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "\n",
    "        # Preprocess texts\n",
    "        preprocessed_texts = [self.preprocessor.preprocess(text) for text in texts]\n",
    "\n",
    "        # Transform to features\n",
    "        X = self.vectorizer.transform(preprocessed_texts)\n",
    "\n",
    "        # Make predictions\n",
    "        model = self.models[model_name]\n",
    "        predictions = model.predict(X)\n",
    "\n",
    "        # Get probabilities if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(X)\n",
    "        else:\n",
    "            probabilities = None\n",
    "\n",
    "        return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb146d19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_simple_demo():\n",
    "    \"\"\"Run a simplified sentiment analysis demo\"\"\"\n",
    "    print(\"üé≠ Social Media Sentiment Analysis - Simple Demo\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Sample tweets for demonstration\n",
    "    sample_tweets = [\n",
    "        \"I absolutely love this new product! It's amazing! üòç\",\n",
    "        \"This is the worst service I've ever experienced. Terrible!\",\n",
    "        \"The weather is okay today, nothing special.\",\n",
    "        \"Best purchase I've made this year! Highly recommend! üåü\",\n",
    "        \"Not impressed with the quality. Could be better.\",\n",
    "        \"Feeling great today! Life is beautiful! ‚òÄÔ∏è\",\n",
    "        \"The movie was boring and too long. Waste of time.\",\n",
    "        \"Pretty good overall experience. Satisfied with the results.\",\n",
    "        \"Absolutely hate this new update. Ruined everything!\",\n",
    "        \"Amazing customer service! They went above and beyond!\",\n",
    "        \"This is confusing and hard to use.\",\n",
    "        \"Love the new features! Great job! üéâ\",\n",
    "        \"It's fine, works as expected.\",\n",
    "        \"Incredible performance! Exceeded expectations!\",\n",
    "        \"Terrible experience. Very disappointed.\"\n",
    "    ]\n",
    "\n",
    "    # Expected labels (0=negative, 1=neutral, 2=positive)\n",
    "    expected_labels = [2, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 0]\n",
    "\n",
    "    print(\"üìù Sample Tweets for Analysis:\")\n",
    "    sentiment_names = ['üòû Negative', 'üòê Neutral', 'üòä Positive']\n",
    "    for i, (tweet, label) in enumerate(zip(sample_tweets, expected_labels), 1):\n",
    "        print(f\"{i:2d}. [{sentiment_names[label]}] {tweet}\")\n",
    "\n",
    "    print(f\"\\nüîß Initializing sentiment analyzer...\")\n",
    "    analyzer = SimpleSentimentAnalyzer()\n",
    "\n",
    "    print(\"üìä Preparing training data...\")\n",
    "    X, y = analyzer.prepare_data(sample_tweets, expected_labels)\n",
    "    print(f\"‚úÖ Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "    print(\"\\nüöÄ Training models...\")\n",
    "    analyzer.train_models(X, y)\n",
    "    print(\"‚úÖ All models trained successfully!\")\n",
    "\n",
    "    print(\"\\nüéØ Testing model performance...\")\n",
    "    models_to_test = ['naive_bayes', 'svm', 'logistic_regression']\n",
    "\n",
    "    results = {}\n",
    "    for model_name in models_to_test:\n",
    "        predictions, probabilities = analyzer.predict(sample_tweets, model_name)\n",
    "        accuracy = accuracy_score(expected_labels, predictions)\n",
    "        results[model_name] = {\n",
    "            'predictions': predictions,\n",
    "            'accuracy': accuracy,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        print(f\"üìä {model_name.replace('_', ' ').title():20s}: Accuracy = {accuracy:.1%}\")\n",
    "\n",
    "    print(\"\\nüìà Detailed Predictions Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, tweet in enumerate(sample_tweets):\n",
    "        print(f\"\\nTweet {i+1}: {tweet[:60]}{'...' if len(tweet) > 60 else ''}\")\n",
    "        print(f\"Expected: {sentiment_names[expected_labels[i]]}\")\n",
    "\n",
    "        for model_name, result in results.items():\n",
    "            prediction = result['predictions'][i]\n",
    "            predicted_sentiment = sentiment_names[prediction]\n",
    "\n",
    "            if result['probabilities'] is not None:\n",
    "                confidence = np.max(result['probabilities'][i]) * 100\n",
    "                status = \"‚úÖ\" if prediction == expected_labels[i] else \"‚ùå\"\n",
    "                print(f\"  {status} {model_name.replace('_', ' ').title():15s}: {predicted_sentiment} ({confidence:.1f}% confidence)\")\n",
    "            else:\n",
    "                status = \"‚úÖ\" if prediction == expected_labels[i] else \"‚ùå\"\n",
    "                print(f\"  {status} {model_name.replace('_', ' ').title():15s}: {predicted_sentiment}\")\n",
    "\n",
    "    print(\"\\nüèÜ Model Performance Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    for rank, (model_name, result) in enumerate(sorted_models, 1):\n",
    "        print(f\"{rank}. {model_name.replace('_', ' ').title():20s}: {result['accuracy']:.1%}\")\n",
    "\n",
    "    print(f\"\\nüéâ Demo completed successfully!\")\n",
    "    print(\"\\nWhat this demo showed:\")\n",
    "    print(\"‚Ä¢ Text preprocessing and cleaning\")\n",
    "    print(\"‚Ä¢ TF-IDF feature extraction\")\n",
    "    print(\"‚Ä¢ Training multiple ML models (Naive Bayes, SVM, Logistic Regression)\")\n",
    "    print(\"‚Ä¢ Model comparison and performance evaluation\")\n",
    "    print(\"‚Ä¢ Confidence scoring for predictions\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07befdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Simple Social Media Sentiment Analysis Demo...\")\n",
    "    print(\"This simplified version uses only basic ML models and dependencies.\\n\")\n",
    "\n",
    "    try:\n",
    "        results = run_simple_demo()\n",
    "\n",
    "        print(\"\\nüí° Next Steps:\")\n",
    "        print(\"1. Install additional dependencies (spaCy, TensorFlow) for advanced features\")\n",
    "        print(\"2. Set up Twitter API credentials for real-time analysis\")\n",
    "        print(\"3. Start the web application:\")\n",
    "        print(\"   - Backend: python backend/api/flask_app.py\")\n",
    "        print(\"   - Frontend: cd frontend && npm start\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚õî Demo interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Demo failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\nüëã Thanks for trying the Social Media Sentiment Analysis Platform!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
